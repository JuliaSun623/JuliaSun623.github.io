<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Covolutional Neural Network - Yu Sun</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="Yu Sun" /><meta name="description" content="为什么在图像上使用CNN CNN v.s. DNN CNN一般用作影像处理，把图片转化成pixel，输出的是N个class。第1个layer是最底层的特征，越往后" /><meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.80.0 with theme even" />


<link rel="canonical" href="http://localhost:1313/post/cnn/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.2e81bbed97b8b282c1aeb57488cc71c8d8c8ec559f3931531bd396bf31e0d4dd.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="Covolutional Neural Network" />
<meta property="og:description" content="为什么在图像上使用CNN CNN v.s. DNN CNN一般用作影像处理，把图片转化成pixel，输出的是N个class。第1个layer是最底层的特征，越往后" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/post/cnn/" />
<meta property="article:published_time" content="2021-02-01T13:51:49+08:00" />
<meta property="article:modified_time" content="2021-02-05T13:51:49+08:00" />
<meta itemprop="name" content="Covolutional Neural Network">
<meta itemprop="description" content="为什么在图像上使用CNN CNN v.s. DNN CNN一般用作影像处理，把图片转化成pixel，输出的是N个class。第1个layer是最底层的特征，越往后">
<meta itemprop="datePublished" content="2021-02-01T13:51:49+08:00" />
<meta itemprop="dateModified" content="2021-02-05T13:51:49+08:00" />
<meta itemprop="wordCount" content="8630">



<meta itemprop="keywords" content="机器学习,NLP,深度学习,CNN," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Covolutional Neural Network"/>
<meta name="twitter:description" content="为什么在图像上使用CNN CNN v.s. DNN CNN一般用作影像处理，把图片转化成pixel，输出的是N个class。第1个layer是最底层的特征，越往后"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Yu Sun&#39;s Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Yu Sun&#39;s Blog</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Covolutional Neural Network</h1>

      <div class="post-meta">
        <span class="post-time"> 2021-02-01 </span>
        <div class="post-category">
            <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%9D%8E%E5%AE%8F%E6%AF%85/"> 机器学习李宏毅 </a>
            </div>
          <span class="more-meta"> 约 8630 字 </span>
          <span class="more-meta"> 预计阅读 18 分钟 </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#为什么在图像上使用cnn">为什么在图像上使用CNN</a>
          <ul>
            <li><a href="#cnn-vs-dnn">CNN v.s. DNN</a></li>
            <li><a href="#cnn理论基础的三个特性">CNN理论基础的三个特性</a></li>
          </ul>
        </li>
        <li><a href="#cnn架构">CNN架构</a>
          <ul>
            <li><a href="#convolution">Convolution</a></li>
            <li><a href="#max-pooling">Max Pooling</a></li>
            <li><a href="#flatten">Flatten</a></li>
          </ul>
        </li>
        <li><a href="#cnn学到了什么">CNN学到了什么</a>
          <ul>
            <li><a href="#filter做了什么">Filter做了什么</a></li>
            <li><a href="#neuron做了什么">Neuron做了什么</a></li>
            <li><a href="#cnn的输出">CNN的输出</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <h2 id="为什么在图像上使用cnn">为什么在图像上使用CNN</h2>
<h3 id="cnn-vs-dnn">CNN v.s. DNN</h3>
<p>CNN一般用作影像处理，把图片转化成pixel，输出的是N个class。第1个layer是最底层的特征，越往后的layer的特征越精确。但是<strong>当我们直接用一般的fully connected的feedforward network来做图像处理的时候，往往会需要太多的参数</strong>。CNN就是用来简化NN的架构，我们利用对知识和图像处理的理解，一开始就把某些实际上用不到的参数给过滤掉，而不用fully connected network，所以CNN其实是比一般的DNN还要更简单的</p>
<p>虽然CNN看起来运作比较复杂，但事实上，它的模型比DNN还要更简单。我们就是用prior knowledge，去把原来fully connected的layer里面的一些参数拿掉，就变成CNN。</p>
<h3 id="cnn理论基础的三个特性">CNN理论基础的三个特性</h3>
<p>为什么可以拿掉其中一些参数而只使用比较少的参数？</p>
<h4 id="一些pattern比整个图片小很多">一些pattern比整个图片小很多</h4>
<p>在图像处理中，假设在network的第一层hidden layer里的neuron要做的事情是侦测有没有一种pattern（图案样式）出现，那大部分的pattern其实是比整张image要小的。所以对一个neuron来说，想要侦测有没有某一个pattern出现，它其实并不需要看整张image，只需要看这张image的一小部分，就可以决定这件事情了。所以，<strong>每一个neuron其实只要连接到一个小块的区域就好，而不需要连接到整张完整的图，因此也对应着更少的参数</strong>。</p>
<p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/pattern.png/" alt="img" style="zoom:40%;" /></p>
<h4 id="相同的pattern会出现在图片的不同位置">相同的pattern会出现在图片的不同位置</h4>
<p>同样的pattern，可能会出现在image的不同部分，但是它们有同样的形状、代表的是同样的含义，因此它们也可以用同样的neuron、同样的参数，被同一个detector检测出来。所以<strong>我们可以要求这些功能几乎一致的neuron共用一组参数，它们share同一组参数就可以帮助减少总参数的量</strong>。</p>
<p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/pattern-region.png/" alt="img" style="zoom:40%;" /></p>
<h4 id="对图片二次抽样不会改变object">对图片二次抽样不会改变object</h4>
<p>如果对一张image做subsampling（二次抽样），例如把奇数行、偶数列的pixel拿掉，image就可以变成原来的十分之一大小，而且并不会影响人对这张image的理解。对你来说，下面两张大小不一的image看起来不会有什么太大的区别，你都可以识别里面有什么物件。因此subsampling对图像辨识来说，可能是没有太大的影响的。所以，<strong>我们可以利用subsampling这个概念把image变小，从而减少需要用到的参数量</strong>。</p>
<p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/subsamp.png/" alt="img" style="zoom:40%;" /></p>
<h2 id="cnn架构">CNN架构</h2>
<ol>
<li>首先，输入一张image以后，它会先通过<strong>Convolution</strong>层，接下来做<strong>Max Pooling</strong>，然后再去做Convolution，再做Maxi Pooling……这个process可以反复进行多次（重复次数需要事先决定），这就是network的架构。就好像network有几层一样，你要做几次convolution，做几次Max Pooling，在定这个network的架构时就要事先决定好。</li>
<li>当完成先前决定的convolution和max pooling的次数后，接下来是是<strong>Flatten</strong>。做完flatten以后，你就把Flatten的output丢到一般的Fully connected network里面，最终得到图像辨识的结果。</li>
</ol>
<p>前面提到CNN对三个property，其中那<strong>前两个property，是用convolution的layer来处理的；最后的property，是用max pooling来处理的</strong></p>
<p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/property.png/" alt="img" style="zoom:40%;" /></p>
<h3 id="convolution">Convolution</h3>
<p>假设network的input是一张6*6的image，图像是黑白的，因此每个pixel只需要用一个value来表示。而在convolution layer里面，有一堆<strong>Filter</strong>，这里每一个Filter，其实就等同于是Fully connected layer里的一个neuron。</p>
<h4 id="property-1">Property 1</h4>
<p>每一个Filter是一个matrix，每个matrix里面的每一个element，本质上与neuron的weight和bias一样，是network的parameter，它们具体的值都是通过Training data学出来的，而不是人去设计的。</p>
<p>图中每一个filter是3*3的大小，意味着它在侦测一个3*3的pattern，<strong>当它侦测的时候，并不会去看整张image，它只看一个3*3范围内的pixel，就可以判断某一个pattern有没有出现，这就考虑了property 1</strong>。</p>
<p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/filter.png/" alt="img" style="zoom:40%;" /></p>
<h4 id="property-2">Property 2</h4>
<p>这个filter是从image的左上角开始，做一个slide window（滑动窗口）。每次向右挪动一定的距离，这个距离就叫做<strong>stride</strong>，由你自己设定。每次filter停下的时候就跟image中对应的3*3的matrix做一个内积。这里假设stride=1，那么我们的filter每次移动一格，当它碰到image最右边的时候，就从下一行的最左边开始重复进行上述操作，经过一整个convolution的process，最终得到下图所示的红色的4*4 matrix。</p>
<p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/filter1.png/" alt="img" style="zoom:40%;" /></p>
<p>观察上图中的Filter1，它斜对角的地方是1,1,1，所以它的工作就是detect有没有连续的从左上角到右下角的1,1,1出现在这个image里面，检测到的结果已在上图中用蓝线标识出来。此时filter得到的卷积结果的左上和左下得到了最大的值，这就代表该filter所要侦测的pattern出现在image的左上角和左下角。</p>
<p>由此可以看出，<strong>同一个pattern出现在image左上角的位置和左下角的位置，并不需要用到不同的filter，我们用filter1就可以侦测出来，这就考虑了property 2</strong>。</p>
<h4 id="feature-map">Feature Map</h4>
<p>在一个convolution的layer里面，会有很多filter，不一样的filter会有不一样的参数，但是这些filter做卷积的过程都是一样的。把filter2跟image做完convolution以后，会得到另外一个蓝色的4*4 matrix，那这个蓝色的4*4 matrix跟之前红色的4*4matrix合起来，就叫做<strong>Feature Map（特征映射）</strong>，有多少个filter，对应就有多少个映射后的image。</p>
<p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/filter2.png/" alt="img" style="zoom:40%;" /></p>
<p>CNN对不同scale的相同pattern的处理上存在一定的困难，由于现在每一个filter size都是一样的，这意味着，如果你今天有同一个pattern，它有不同的size，有大的鸟嘴，也有小的鸟嘴，CNN并不能够自动处理这个问题。</p>
<h4 id="colorful-image">Colorful Image</h4>
<p>刚才举的例子是黑白的image，所以input是一个matrix；如果输入是彩色的image，即图像由RGB组成的，所以它就是好三个matrix叠在一起，是一个立方体。这个时候filter就不再是一个matrix了，它也会是一个立方体。如果你今天是RGB这三个颜色来表示一个pixel的话，那你的input就是3*6*6，filter就是3*3*3，filter的高就是3。你在做convolution的时候，就是立方体和立方体做内积。</p>
<p>⚠️ 并不是把每个channel（矩阵层）分开算，而是9个值合起来做内积。就是把这个filter的9个值跟这个image里面的9个值做内积，可以想象成filter的<strong>每一层都分别跟image的三层</strong>做内积，得到的也是一个三层的output，每一个filter同时就考虑了不同颜色所代表的channel。</p>
<p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/rgb.png/" alt="img" style="zoom:40%;" /></p>
<h4 id="convolution-vs-fully-connected">Convolution v.s. Fully Connected</h4>
<p>convolution其实本质就是一个neural network。</p>
<h5 id="filter就是特殊的neuron">Filter就是特殊的“neuron”</h5>
<p>convolution其实就是fully connected的layer把一些weight拿掉而已。feature map的output，其实就是hidden layer的neuron的output。</p>
<p>如下图所示，在做convolution的时候，把filter放在image的左上角，然后再去做inner product，得到一个值3。这件事情就相当于，把这个image的6*6的matrix拉直变成右边这个用于input的36维的vector。然后，有一个红色的neuron，这些input经过这个neuron之后，得到的output是3。</p>
<p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/filter-neuron1.png/" alt="img" style="zoom:40%;" /></p>
<h5 id="每个neuron只检测image部分区域property-1">每个neuron只检测image部分区域（Property 1）</h5>
<p>上面提到的neuron实际上就是由filter转化而来的。我们把filter放在image的左上角，此时filter考虑的就是和它重合的9个pixel。假设把这一个6*6的image的36个pixel拉成直的vector作为input，那这9个pixel分别就对应着右侧编号1，2，3的pixel，编号7，8，9的pixel跟编号13，14，15的pixel。</p>
<p>所以在上图中，input vector经过某个neuron得到的output 3，就代表存在这样一个neuron，这个neuron带weight的连线只连接到编号为1，2，3，7，8，9，13，14，15的这9个pixel，而这个neuron和这9个pixel连线上所标注的的weight就是filter matrix里面的这9个数值。</p>
<p>作为对比，Fully connected的neuron是必须连接到所有36个input上的，但是Convolution中只用连接9个input。这是因为我们知道要detect一个pattern，不需要看整张image，看9个input pixel就够了，因此使用了比较少的参数。</p>
<h5 id="neuron之间共享参数property-2">neuron之间共享参数（Property 2）</h5>
<p>当我们把filter做stride = 1的移动的时候，通过filter和image matrix的内积会得到另一个output值-1，我们假设这个-1是另外一个neuron的output，此时这个neuron会连接到到pixel 2，3，4，pixel 8，9，10和pixel 14，15，16。</p>
<p>这两个output为3和-1的neuron，它们分别去检测在image的两个不同位置上是否存在某个pattern，因此在Fully connected layer里它们做的是两件不同的事情，每一个neuron应该有自己独立的weight；但是，当我们做convolution的时候，我们强迫某些neuron（比如下图中output为3和-1的两个neuron）一定要共享一组weight。虽然这两个neuron连接到的pixel对象各不相同，但它们用的weight都必须是一样的，都等于filter里面的元素值，这件事情就叫做weight share。此时，Network用的参数又会比原来更少。</p>
<p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/share-weight.png/" alt="img" style="zoom:40%;" /></p>
<h4 id="总结">总结</h4>
<p>因此我们可以这样理解，有这样一些特殊的neuron，它们只连接着9条带weight的线（9=3*3对应着filter中的元素个数，这些weight也就是filter内部的元素值，上图中圆圈的颜色与连线的颜色一一对应）。</p>
<p>当filter在image matrix上移动做convolution的时候，每次移动做的事情实际上是去检测这个地方有没有某一种pattern。对于Fully connected layer来说，它是对整张image做detection的，因此每次检测上不同地方其实是不同的事情，所以这些neuron都必须连接到整张image的所有pixel上，并且不同neuron的连线上的weight都是相互独立的。</p>
<p><strong>但对于convolution layer来说，它是对image的一部分做detection的，因此它的neuron只需要连接到image的部分pixel上，对应连线所需要的weight参数就会减少。另外，由于是用同一个filter去检测不同位置的pattern，所以这其实是完成同一件事情。因此虽然连接到的pixel对象各不相同，但是在用同一个filter的前提下，这些neuron所使用的weight参数都是相同的。通过这样一张weight share的方式，再次减少network所需要用到的weight参数</strong></p>
<p>CNN的本质，其实就是减少参数的过程。</p>
<h3 id="max-pooling">Max Pooling</h3>
<p>相较于convolution，max pooling是比较简单的，它就是做subsampling。根据filter 1，我们得到一个4*4的matrix，根据filter 2，得到另外一个4*4的matrix，接下来，把output四个分为一组，每一组里面通过选取平均值或最大值的方式，把原来4个value合成一个 value。这相当于在image每相邻的四块区域内都挑出一块来检测，这种subsampling的方式就可以让image缩小。</p>
<p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/max-pooling.png/" alt="img" style="zoom:40%;" /></p>
<p>做完一次convolution加一次max pooling，我们就把原来6<em>6的image，变成了一个2</em>2的image。这个2*2的image的每一个pixel的深度，即每一个pixel用几个value来表示，就取决于有几个filter。</p>
<p>所以，得到的新的比较小的image，表示的是不同区域上提取到的特征。实际上不同的filter检测的是该image同一区域上的不同特征属性，所以每一层channel代表的是一种属性，一块区域有几种不同的属性，就有几层不同的channel，对应的就会有几个不同的filter对其进行convolution操作。</p>
<p>⚠️$N$个filter， 做完一次convolution，得到$N$个feature map之后再做一次convolution，还是会得到$N$个feature map。因为convolution在考虑input的时候，是会考虑深度的，它并不是每一个channel分开考虑，而是<strong>一次考虑所有的channel</strong>，所以，convolution这边有多少个filter，再次output的时候就会有多少个channel。</p>
<h3 id="flatten">Flatten</h3>
<p>做完convolution和max pooling之后，就是FLatten和Fully connected Feedforward network的部分。</p>
<p>Flatten的意思是把左边的feature map拉直，然后把它放进一个Fully connected Feedforward network，然后就结束了。因此，我们之前通过CNN提取出了image的feature，它相较于原先一整个image的vetor，少了很大一部分内容，因此需要的参数也大幅度地减少了，但最终，也还是要使用一个Fully connected的network中去做最后的分类工作。</p>
<p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/fatten.png" alt="img" style="zoom:40%;" /></p>
<h2 id="cnn学到了什么">CNN学到了什么</h2>
<p>大家常说deep learning就是一个黑盒子，你学习出来以后，并不知道为什么是这样子，于是你会感觉它很intelligent，但是其实还是有很多方法可以分析的，今天我们就来示范一下怎么分析CNN，看一下它到底学到了什么。</p>
<h3 id="filter做了什么">Filter做了什么</h3>
<p>要分析第一个convolution的filter是比较容易的，因为第一个convolution layer里面，每一个filter就是一个3*3的matrix，它对应到3*3范围内的9个pixel，所以只要看这个filter的值，就可以知道它在detect什么东西，因此第一层的filter是很容易理解的。</p>
<p>但是第二层的filter比较难理解，它们是50个同样为3*3的filter，但是这些filter的input并不是pixel，而是做完convolution再做Max pooling的结果。因此filter考虑的范围并不是3*3=9个pixel，而是一个长宽为3*3，高为25的cubic，filter实际在image上看到的范围是远大于9个pixel的，所以就算把它的weight拿出来，也不知道它在做什么。</p>
<p>假设在第二个convolution layer里面的50个filter，每一个filter的output就是一个11*11的matrix，假设现在把第k个filter的output拿出来，如下图所示，这个matrix里的每一个element，标记为$a_{ij}^k$，上标k表示这是第k个filter，下标ij表示它在这个matrix里的第i个row，第j个column</p>
<p>接下来定义一个$a^k$叫做<strong>Degree of the activation of the k-th filter</strong>，这个值表示现在的第k个filter，它有多被activate，有多被“启动”，直观来讲就是描述现在input的东西跟第k个filter有多接近，它对filter的激活程度有多少。</p>
<p>第k个filter被启动的degree 就定义成，它与input进行卷积所输出的output里所有element的summation，以上图为例，就是这11*11的output matrix里所有元素之和，$a^k=\sum_{i=1}^{11}\sum_{j=1}^{11}a_{ij}^k$。</p>
<p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/kth-filter.png/" alt="img" style="zoom:40%;" /></p>
<p>也就是说，我们input一张image，然后把这个filter和image进行卷积所得到的11*11个值全部加起来，当作现在这个filter被activate的程度。</p>
<p>现在我们想要知道第k个filter的作用是什么，那就要找一张image，这张image可以让第k个filter被activate的程度最大，即：$x^*=arg\ max_xa^k$。之前求minimize用的是gradient descent，那现在我们求Maximum用gradient ascent(梯度上升法)就可以了。</p>
<p>可以理解为，现在是把input x作为要找的参数，对它去用gradient descent或ascent进行update。原来在train CNN的时候，input是固定的，model的参数是要用gradient descent去找出来的；但是现在这个立场是反过来的，在这个task里面model的参数是固定的，我们要用gradient ascent去update这个x，让它可以使degree of activation最大。</p>
<p>下图就是得到的结果，50个filter理论上可以分别找50张image使对应的activation最大，这里仅挑选了其中的12张image作为展示。这些image有一个共同的特征，它们里面都是一些反复出现的某种texture(纹路)。比如说第三张image上布满了小小的斜条纹，这意味着第三个filter的工作就是detect图上有没有斜条纹。因为现在每个filter检测的都只是图上一个小范围，所以图中一旦出现一个小小的斜条纹，这个filter就会被activate，相应的output也会比较大。所以如果整张image上布满这种斜条纹的话，这个时候它会最兴奋，filter的activation程度是最大的，相应的output值也会达到最大。</p>
<p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/gradient-ascent.png/" alt="img" style="zoom:40%;" /></p>
<p>因此每个filter的工作就是去detect某一种pattern，detect某一种线条，上图所示的filter所detect的就是不同角度的线条。所以今天input有不同线条的话，某一个filter会去找到让它兴奋度最高的匹配对象，这个时候它的output就是最大的。</p>
<h3 id="neuron做了什么">Neuron做了什么</h3>
<p>想要知道在这个neural network里的每一个neuron是做什么的，可以模仿上面的做法。</p>
<p>我们定义第j个neuron的output就是$a_j$，接下来就用gradient ascent的方法去找一张image x，把它输入到neural network里面就可以让$a_j$的值被maximize，即$x^*=arg\ max_xa^j$。</p>
<p>找到的结果如下图所示，同理这里仅取出其中的9张image作为展示，发现这9张图跟之前filter所观察到的情形是不一样的。刚才我们观察到的是类似纹路的东西，那是因为每个filter考虑的只是图上一部分的vision，所以它detect的是一种texture。但是在做完Flatten以后，每一个neuron不再是只看整张图的一小部分，它现在的工作是看整张图，所以对每一个neuron来说，让它最兴奋的、activation最大的image，不再是texture，而是一个完整的图形。</p>
<p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/neuron-do.png/" alt="img" style="zoom:40%;" /></p>
<h3 id="cnn的输出">CNN的输出</h3>
<p>接下来我们考虑的是CNN的output，由于是手写数字识别的demo，因此这里的output就是10维，我们把某一维拿出来，然后同样去找一张image x，使这个维度的output值最大，即$x^*=arg\ max_xy^j$。</p>
<p>找到结果如下图，每一张图分别对应着数字0-8。我们发现可以让数字1对应neuron的output值最大的image其实长得一点也不像1。为了验证，这里又做了一个实验，把上述得到的image真的作为testing data输入到CNN里面，结果classify的结果确实还是认为这些image就对应着数字0-8。</p>
<p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/cnn-output.png/" alt="img" style="zoom:40%;" /></p>
<p>那我们有没有办法让上面这个图看起来更像数字呢？所以我们要对这个x做一些regularization，我们要对找出来的x做一些限制约束，我们应该告诉机器说，虽然有一些x可以让输出的y很大，但是它们不是数字。假设image里的每一个pixel都用$x_{ij}$表示，我们把所有pixel值取绝对值并求和，也就是，这一项其实就是之前提到过的L1的regularization，再用$y^i$减去这一项，得到$x^*=arg\ max_x(y^i-\sum_{i,j}|x_{ij}|)$。</p>
<p>这次我们希望再找一个input x，它可以让最大的同时，也要让的summation越小越好，也就是说我们希望找出来的image，大部分的地方是没有涂颜色的，只有少数数字笔画在的地方才有颜色出现。得到结果如下：</p>
<p><img src="https://gitee.com/Sakura-gh/ML-notes/raw/master/img/L1.png/" alt="img" style="zoom:40%;" /></p>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content">Yu Sun</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        2021-02-05
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
          <a href="/tags/nlp/">NLP</a>
          <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
          <a href="/tags/cnn/">CNN</a>
          </div>
      <nav class="post-nav">
        
        <a class="next" href="/post/backprop/">
            <span class="next-text nav-default">Backpropagation</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:cnsdytsy@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="https://stackoverflow.com/users/15074966/ysun" class="iconfont icon-stack-overflow" title="stack-overflow"></a>
      <a href="https://github.com/JuliaSun623" class="iconfont icon-github" title="github"></a>
      <a href="https://www.instagram.com/chubbyfishfish/" class="iconfont icon-instagram" title="instagram"></a>
  <a href="http://localhost:1313/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2017 - 
    2021<span class="heart"><i class="iconfont icon-heart"></i></span><span>olOwOlo</span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.c12618f9a600c40bd024996677e951e64d3487006775aeb22e200c990006c5c7.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>








</body>
</html>
